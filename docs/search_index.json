[["index.html", "Tutorial Note for Bayesian Optimization Chapter 1 Bayesian Optimization Workflow 1.1 Overall View 1.2 Gaussian Process", " Tutorial Note for Bayesian Optimization Peyman Kor 2021-08-27 Chapter 1 Bayesian Optimization Workflow 1.1 Overall View Bayesian Optimization (BO) is an optimization method that builds a probabilistic model to mimic an expensive objection function. The probabilistic model is an inference from a finite number of function evaluations. This finite number of evaluations is done as initialization of the workflow and build a probabilistic model. After initializing and building a probabilistic model, a new query point is evaluated using the expensive objective function at each iteration. Then the new data \\((\\mathbf{u}^{new},\\mathbf{J}(\\mathbf{u}^{new}))\\) is assimilated back to the probabilistic model to update the model. The unique methodology of using a non-deterministic surrogate model makes Bayesian optimization (BO) an efficient global optimizer capable of exploring and exploiting space of decision. In the rest of this section, the objective function is shown with \\(\\overline{\\mathbf{J}}(\\mathbf{u})\\), consistent with the Equation (??). However, for convention, we drop the bar and write the \\(\\overline{\\mathbf{J}}(\\mathbf{u})\\) with \\(\\mathbf{J}(\\mathbf{u})\\). Moreover, \\(\\mathbf{u}\\) is a control decision, with a dimension of \\(D\\), \\(\\mathbf{u}=[u_1,\\cdots,u_D]\\). While, the capital letter, \\(\\mathbf{U}\\) is collection of \\(\\mathbf{N}\\) points of \\(\\mathbf{u}\\), defined as: \\(\\mathbf{U}= [\\mathbf{u_1},\\cdots,\\mathbf{u_N}]\\). The workflow of BO can be divided into two steps: Step 1: Choose some initial design points \\(\\mathcal{D}=\\{{\\mathbf{U},\\mathbf{J(U)}}\\}\\) to build a probabilistic model inferred from \\(\\mathcal{D}\\) Step 2: Deciding on next \\(\\mathbf{u}^{next}\\) and evaluate \\(\\mathbf{J(u^{next})}\\) based on probabilistic model and \\(\\mathcal{D}=\\mathcal{D}\\: \\cup[\\mathbf{u}^{next},\\mathbf{J(u^{next})}]\\) After step 2, we come back to step 1 with the new \\(\\mathcal{D}\\), and we iterate this process until we are out of computational budget. First, we will explain Gaussian Process (GP) as a method for building a probabilistic model as a background for the workflow. Then, both steps are explained in detail. 1.2 Gaussian Process In this work, we employ the widely used Gaussian process (GP) as a probabilistic model. Known as a surrogate model (since it tries to mimic the real, expensive objective function), GP is an attractive choice because it is computationally traceable with the capability to quantify the uncertainty of interest (Rasmussen and Williams 2006; Murphy 2022). A GP can be seen as an extension of the Gaussian distribution to the functional space. Key assumption in (GP) is that: the function values at a set of \\(M &gt; 0\\) inputs, \\(\\mathbf{J} = [\\mathbf{J({u_1})}, ...,\\mathbf{J(u_M)}]\\), is jointly Gaussian, with mean and Covariance defined as: \\[\\begin{equation} \\begin{split} &amp; \\mathbb{E} \\: [\\mathbf{J(u)}]= m(\\mathbf{u}) \\\\ &amp; \\text{Cov} \\: [\\mathbf{J(u)}),J(\\mathbf{J(u&#39;)}]= \\kappa(\\mathbf{u},\\mathbf{u&#39;}) \\end{split} \\tag{1.1} \\end{equation}\\] In (1.1), \\(m(\\mathbf{u})\\) is a mean function and \\(\\kappa(\\mathbf{u},\\mathbf{u&#39;})\\) is a covariance function (or kernel). \\(\\kappa(\\mathbf{u},\\mathbf{u&#39;})\\) specifies the similarity between two values of a function evaluated on \\(\\mathbf{u}\\), and \\(\\mathbf{u&#39;}\\) . A GP is a distribution over function completely defined by its mean and covariance function as: \\[\\begin{equation} J(\\mathbf{u}) \\sim \\mathcal{N}(m(\\mathbf{u}), \\kappa(\\mathbf{u},\\mathbf{u&#39;})) \\tag{1.2} \\end{equation}\\] where \\(\\mathcal{N}\\) denotes a multivariate normal distribution.As discussed in (Shahriari et al. 2016), there are many choices for the covariance function; the most commonly used ones in the literature have been depicted in Table 1.1. Table 1.1: Several types of covariance function for the GP process Covariance Kernels assumeing \\(h=||u-u&#39;||\\) Gaussain \\(\\Large \\kappa (\\mathbf{u},\\mathbf{u&#39;}) =\\sigma_f^2 exp(-\\frac{h^2}{2\\ell^2})\\) Matern \\(\\nu=\\frac{5}{2}\\) \\(\\Large \\kappa (\\mathbf{u},\\mathbf{u&#39;}) =\\sigma_f^2(1 + \\frac{\\sqrt{5}|h|}{\\ell}\\frac{5h^2}{3\\ell^2})exp(-\\frac{ -\\sqrt{5}|h|}{\\ell})\\) Matern \\(\\nu=\\frac{3}{2}\\) \\(\\Large \\kappa (\\mathbf{u},\\mathbf{u&#39;}) =\\sigma_f^2(1 + \\frac{\\sqrt{3}|h|}{\\ell})exp(-\\frac{-\\sqrt{3}|h|}{\\ell})\\) Exponetial \\(\\Large \\kappa (\\mathbf{u},\\mathbf{u&#39;}) =\\sigma_f^2 exp(-\\frac{|h|}{\\ell})\\) Power-Exponetial \\(\\Large \\kappa (\\mathbf{u},\\mathbf{u&#39;}) =\\sigma_f^2 exp(-(\\frac{|h|}{\\ell})^p)\\) Where in the Table 1.1, \\(\\ell\\) is length-scale, and \\(h\\) is eludian distance of \\(\\mathbf{u}\\), \\(\\mathbf{u&#39;}\\). ( Note that \\(|h|^2=(\\mathbf{u}-\\mathbf{u&#39;})^\\intercal(\\mathbf{u}-\\mathbf{u&#39;})\\)). In this work, the Matern covariance function with \\(\\nu=\\frac{5}{2}\\) was employed. However, depending to any choice of covariance function, the parameters of covariance function needs to be estimated. These parameters can be denoted as \\(\\theta\\) as: \\[\\begin{equation} \\theta = [\\sigma^2_{f},\\ell] \\tag{1.3} \\end{equation}\\] The parameter \\(\\theta\\) needs to be optimized, as it will be explained later. With this background, BO workflow is explained as follows. 1.2.1 Step 1: Choose some initial design points \\(\\mathcal{D}=\\{{\\mathbf{U},\\mathbf{J(U)}}\\}\\) to build probabilistic model inferred from \\(\\mathcal{D}\\) Assuming we start GP with a finite number of an initial evaluation of \\(\\mathbf{J(u)}\\) on the points in \\(\\mathbf{U}\\), we can define the data-set \\(\\mathcal{D}\\) as: \\[\\begin{align} \\begin{split} \\mathbf{U}= &amp; \\: [\\mathbf{u_1},\\cdots,\\mathbf{u_N}] \\\\ \\mathbf{J_U}= &amp; \\: [\\mathbf{J(u_1)},\\cdots,\\mathbf{J(u_N)}] \\\\ \\mathcal{D}= &amp; \\: \\{\\mathbf{U},\\mathbf{\\mathbf{J_U}}\\} \\end{split} \\tag{1.4} \\end{align}\\] Now we consider the case of predicting the outputs for new inputs that are not in \\(\\mathcal{D}\\). Specifically, given a test set (prediction set) set \\(\\mathbf{U_*}\\) of size \\(\\mathbf{N_* \\times D}\\), we want to predict the function outputs \\(\\mathbf{J_{U_*}} = [\\mathbf{J(u_1)},\\cdots, \\mathbf{J(u_{N_*})}]\\). By definition of the GP, the joint distribution \\(p(\\mathbf{J_U}, \\mathbf{J_{U_*}})\\) has the following form: \\[\\begin{equation} \\begin{bmatrix} {\\bf {J_U}} \\\\ {\\mathbf{J_{U_*}}} \\end{bmatrix} \\sim \\mathcal{N} \\begin{pmatrix} \\begin{bmatrix} {m(\\mathbf{U})} \\\\ {m(\\mathbf{U_*})} \\end{bmatrix},\\begin{bmatrix} {{\\bf K}_{U,U}} &amp; {{\\bf K}_{U,U_*}} \\\\ {{\\bf \\mathbf{K}^\\intercal}_{U,U_*}} &amp; {{\\bf K}_{U_*,U_*} } \\end{bmatrix}\\end{pmatrix} \\tag{1.5} \\end{equation}\\] Where, \\(m(\\mathbf{U})\\) is prior knowledge about mean value of \\(\\mathbf{J_U}\\), defined as \\(m(\\mathbf{U})=[m(\\mathbf{u_1}),\\cdots,m(\\mathbf{u_N})]\\). For simplicity someone can assume the prior mean function to be zero: \\(m(\\mathbf{U}) = 0\\). This assumption is not restrictive because as more training points are observed the prior is updated and becomes more informative. In this work, we considered the case where the mean function could have a linear trend in the form of: \\[\\begin{equation} m(\\mathbf{u}) = \\sum_{j=1}^p \\beta_j \\mathbf{u} \\tag{1.6} \\end{equation}\\] The Gram matrix, \\(\\mathbf{K}_{U,U}\\), is \\(\\mathbf{N \\times N}\\) matrix, with each element is covariance of \\(\\mathbf{u}\\) and \\(\\mathbf{u&#39;}\\): \\[\\begin{equation} \\mathbf{K}_{U,U}=\\kappa(\\mathbf{U,U})=\\left ( \\begin{array}{ccc} \\begin{array}{l} \\kappa(\\mathbf{u_1},\\mathbf{u_2}) \\end{array} &amp; \\cdots &amp; \\begin{array}{l} \\kappa(\\mathbf{u_1},\\mathbf{u_N}) \\end{array} \\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\begin{array}{l} \\kappa(\\mathbf{u_N},\\mathbf{u_1}) \\end{array} &amp; \\cdots &amp; \\begin{array}{l} \\kappa(\\mathbf{u_N},\\mathbf{u_N}) \\end{array} \\end{array} \\right ) \\tag{1.7} \\end{equation}\\] By the standard rules for conditioning multivariate Gaussian distribution, we can drive the posterior (conditional distribution of \\(\\mathbf{J_{U_*}}\\) given the \\(\\mathcal{D}\\)) in closed form as follows: \\[\\begin{align} \\begin{split} p(\\mathbf{J_{U_*}}|\\mathbf{\\mathcal{D},\\theta)}= &amp; \\: \\mathcal{MN}(\\mathbf{J_{U_*}| \\mathbf{\\mu_*},\\textstyle \\sum_{\\ast}}) \\\\ {\\mathbf{\\mu_\\ast}}= &amp; \\: m(\\mathbf{U_\\ast}) +\\mathbf{K}^\\intercal_{U,U_*} \\mathbf{K}^{-1}_{U,U}(\\mathbf{J_U}-m(\\mathbf{U})) \\\\ \\textstyle \\sum_{\\ast}=&amp; \\: \\normalsize{\\mathbf{K}_{U_\\ast,U_\\ast}-\\mathbf{K}^\\intercal_{U,U_\\ast}\\mathbf{K}_{U,U}^{-1}\\mathbf{K}_{U,U_\\ast}} \\end{split} \\tag{1.8} \\end{align}\\] The conditional probability of the \\(\\mathbf{J_{U_*}}\\) Equation (1.8) is conditioned on \\(\\mathcal{D}\\) meaning the available data points to be inferred, and \\(\\theta\\) which is parameters of covariance function, as shown in Equation. 1.2.1.1 Parameter Estimation of Covariance Kernel As it shown in the 1.1, the Matern Covariance function with \\(\\nu=\\frac{5}{2}\\) has two parameters to be estimated, namely \\(\\sigma^2_f\\) and \\(\\ell\\). GP is fit to the data by optimizing the evidence-the marginal probability of the data given the model with respect to the marginalized kernel parameters. Known as the empirical Bayes approach, we will maximize the marginal likelihood: \\[\\begin{equation} p(\\mathbf{y}|\\mathbf{J_U,\\mathbf{\\theta}})= \\int p(\\mathbf{y}|\\mathbf{J_U})p(\\mathbf{J_U}|\\mathbf{\\theta})d\\mathbf{J} \\tag{1.9} \\end{equation}\\] The term \\(p(\\mathbf{y}|\\mathbf{J_U,\\mathbf{\\theta}})\\) in fact represent the probability of observing the data \\(y\\)given on the model, \\(\\mathbf{J_U,\\mathbf{\\theta}}\\). The reason it is called the marginal likelihood, rather than just likelihood, is because we have marginalized out the latent Gaussian vector \\(\\mathbf{J_U}\\). The \\(log\\) of marginal likelihood then can be written as: \\[\\begin{equation} \\text{log} \\: p(\\mathbf{y}|\\mathbf{J_U,\\mathbf{\\theta}})=\\mathcal{L}(\\sigma_f^2,\\ell)=-\\frac{1}{2}(\\mathbf{y}-m(\\mathbf{U}))^{\\intercal}\\mathbf{K}_{U,U}^{-1}(\\mathbf{y}-m(\\mathbf{U}))-\\frac{1}{2}\\text{log}|\\mathbf{K}_{U,U}|-\\frac{N}{2}log(2\\pi) \\tag{1.10} \\end{equation}\\] Where the dependence of the \\(\\mathbf{K}_{U,U}\\) on \\(\\theta\\) is implicit. This objective function consists of a model fit and a complexity penalty term that results in an automatic Occam’s razor for realizable functions (Rasmussen and Williams 2006). By optimizing the evidence with respect to the kernel hyperparameters, we effectively learn the structure of the space of functional relationships between the inputs and the targets. The gradient-based optimizer is performed in order to: \\[\\begin{equation} \\theta^{\\ast}=[\\sigma_f^{2\\ast}, \\ell^{\\ast}]=argmax \\: \\mathcal{L}(\\sigma^2_f,\\ell) \\tag{1.11} \\end{equation}\\] However, since the objective \\(\\mathcal{L}\\) is not convex, local minima can be a problem, so we need to use multiple restarts. It is useful to note that the value \\(\\theta^{\\ast}\\) could be estimated using only a “initial data,” \\(\\mathcal{D}=[\\mathbf{U},\\mathbf{J_U}]\\). Therefore Equation (1.8) can be written using the “optimized” value of \\(\\theta\\). Moreover, given that in next step usually, we need probability distribution of \\(\\mathbf{J}\\) for each control value (\\(\\mathbf{u}\\)), equation (1.8) can be written as: \\[\\begin{align} \\begin{split} p(\\mathbf{J_{u_*}}|\\mathbf{\\mathcal{D},\\theta^\\ast})= &amp; \\: \\mathcal{N}(\\mathbf{J_{u_*}}| \\mathbf{\\mu_{u_\\ast}}, \\mathbf{\\sigma^2_{u_{\\ast}}}) \\\\ \\mathbf{\\mu_{u_\\ast}}= &amp; \\: m(\\mathbf{u_\\ast}) +\\mathbf{K}^\\intercal_{U,u_*} \\mathbf{K}^{-1}_{U,U}(\\mathbf{J_U}-m(\\mathbf{U})) \\\\ \\textstyle \\sigma^2_{\\mathbf{u_{\\ast}}}=&amp; \\: \\normalsize{\\mathbf{\\kappa}_{u_\\ast,u_\\ast}-\\mathbf{K}^\\intercal_{U,u_\\ast}\\mathbf{K}_{U,U}^{-1}\\mathbf{K}_{U,u_\\ast}} \\end{split} \\tag{1.12} \\end{align}\\] In (1.12), we replaced the \\(\\mathcal{MN}\\) with \\(\\mathcal{N}\\) in (1.8) as Equation (1.12) shows the probability of \\(\\mathbf{J}\\) for one control variable, wherein Equation (1.8) we have th probality of the \\(\\mathbf{J}\\), over a vector of the control variable, \\(\\mathbf{U}\\). 1.2.2 Step.2 Deciding on next \\(\\mathbf{u}^{next}\\) based on the probabilistic model The posterior of the probabilistic model given by Equation (1.8) can quantify the uncertainty over the space of the unknown function, \\(f\\). The question is, what is the next \\(\\mathbf{u}^{next}\\) to feed into the expensive function?. In other words, so far we have \\(\\mathcal{D}\\), but need to decide the next \\(\\mathbf{u}^{next}\\) so that going back to Step 1, our updated \\(\\mathcal{D}\\) be \\(\\mathcal{D}=\\mathcal{D} \\: \\cup[\\mathbf{u^{next}},\\mathbf{J(u^{next})}]\\). One could select the next point arbitrarily, but that would be wasteful. To answer this question, we define a utility function, and the next query point is the point that with maximum utility. The literature of BO has seen many utility functions (called acquisition function in the computer science community). These include the Improvement based policies (Probability of Improvement (PI), Expected Improvement(EI)), optimistic policies (Upper Confidence Bound (UCB)), or Information-based (like Thompson Sampling (TS)). The full review of these utility functions and their strength and weakness could be reviewed in (Shahriari et al. 2016). In the Expected Improvement (EI) policy, the utility is defined as follows: \\[\\begin{equation} utility(\\mathbf{u_\\ast};\\theta^{\\ast},\\mathcal{D})=\\alpha_{EI}(\\mathbf{u_\\ast};\\theta^\\ast,\\mathcal{D})=\\int_{y}^{}max(0,\\mathbf{J_{u_*}}-f)p(\\mathbf{J_{u_*}}|\\mathbf{\\mathcal{D},\\theta^\\ast}) \\,dy \\tag{1.13} \\end{equation}\\] The utility defined in Equation @(ref:utiint) can be seen as the expected value of improvement in posterior of the model (Equation (1.8)) compared to the true function at point \\(\\mathbf{u_\\ast}\\). Note that the term \\(p(\\mathbf{J_{u_*}}|\\mathbf{\\mathcal{D},\\theta^\\ast})\\) inside the integral already has been defined at Equation (1.8). However, we do not have access to the expensive function, \\(f\\); therefore, we replace the \\(f\\) with the best available solution found so far, \\(\\mathbf{J}^+\\). The \\(\\mathbf{J^+}\\) mathematically can be defined simply as below, then Equation (1.13) can be written as Equation (1.15): \\[\\begin{equation} \\begin{aligned} \\mathbf{J^+} = \\; \\underset{\\mathbf{u} \\subseteq \\mathcal{D}}{\\text{max}} \\; \\mathbf{J(u)} \\end{aligned} \\tag{1.14} \\end{equation}\\] \\[\\begin{equation} \\alpha_{EI}(\\mathbf{u_\\ast};\\theta^\\ast,\\mathcal{D})=\\int_{y}^{}max(0,\\mathbf{J_{u_*}}-\\mathbf{J^+})p(\\mathbf{J_{u_*}}|\\mathbf{\\mathcal{D},\\theta^\\ast}) \\,dy \\tag{1.15} \\end{equation}\\] After applying some tedious integration by parts on the right side of (1.15), one can express the expected improvement in a closed form (Jones, Schonlau, and Welch 1998). To achieve closed form, first, we need some parametrization and define the \\(\\gamma(\\mathbf{u_*})\\) as below: \\[\\begin{equation} \\gamma(\\mathbf{u_*})=\\frac{\\mathbf{\\mu_{u_\\ast}}-\\mathbf{J^+}}{\\sigma_\\mathbf{u_{\\ast}}} \\tag{1.16} \\end{equation}\\] Where \\(\\mathbf{\\mu_{u_\\ast}}\\) and \\(\\sigma_\\mathbf{u_{\\ast}}\\) can be found from Eqaution (1.12) and \\(\\mathbf{J^+}\\) has been defined in Equation (1.14). Given the \\(\\gamma(\\mathbf{u_*})\\), the right side of Equation (1.15) can be written as: \\[\\begin{equation} \\alpha_{EI}(\\mathbf{u_*};\\theta^*,\\mathcal{D})=(\\mathbf{\\mu_{u_\\ast}}-\\mathbf{J^+})\\Phi(\\gamma(\\mathbf{u _*})) + \\sigma_{\\mathbf{u_{\\ast}}} \\phi(\\gamma(\\mathbf{u_*})) \\tag{1.17} \\end{equation}\\] Where \\(\\Phi(.)\\) and \\(\\phi(.)\\) are CDF and PDF of standard Gaussian distribution. We need to note that \\(\\alpha_{EI}(\\mathbf{u_*};\\theta^*,\\mathcal{D})\\) is always non-negative, as the integral defined in (1.15) is truncating the negative side of the function \\(\\mathbf{J}\\) inside the \\(max()\\) term. The Equation(1.17) does a fine job in many applications of Bayesian Optimization. However, the utility defined in Equation (1.17) sometimes can be greedy. In this context, greedy utility means that it is focused more on the “immediate reward,” which is the first part of Equation (1.17), less on the “Exploration” part. Therefore to avoid this greed and make the utility function more forward-looking, an explorative term is added as \\(\\epsilon\\), and Equation (1.16) can be re-written as: \\[\\begin{equation} \\gamma(\\mathbf{u_*})=\\frac{\\mathbf{\\mu_{u_\\ast}}-\\mathbf{J^+}-\\epsilon}{\\sigma^2_{\\mathbf{u_{\\ast}}}} \\tag{1.18} \\end{equation}\\] Likewise, Expected improvement (EI) at point \\(\\mathbf{u_*}\\) can be defined then as: \\[\\begin{equation} \\alpha_{EI}(\\mathbf{u_*};\\theta,\\mathcal{D})=(\\mathbf{\\mu_{u_\\ast}}-\\mathbf{J^+}-\\epsilon)\\Phi(\\gamma(\\mathbf{u _*})) + \\sigma_{\\mathbf{u_{\\ast}}} \\phi(\\gamma(\\mathbf{u_*})) \\tag{1.19} \\end{equation}\\] In this work, the utility defined in Equation (1.19) was considered. The data \\(\\mathcal{D}\\) was normalized to the scale of \\([0,1]\\). Given that scaling, \\(\\epsilon=0.1\\) was used in this work. At the end, the answer to the question of the next query is the point where the utility is maximum, can be defined as: \\[\\begin{equation} \\mathbf{u}_*^{next}=\\underset{\\mathbf{u_*} \\in \\mathbf{U_*} }{\\mathrm{argmax}} \\; \\alpha_{EI}(\\mathbf{u_*};\\theta,\\mathcal{D}) \\tag{1.20} \\end{equation}\\] The Equation in (1.20) represents a need for internal optimization in each iteration of BO. However, worth noting that the optimization of Equation (1.20) is not computationally difficult for two main reasons. First, the forward evaluation of the Equation (1.20), \\(\\alpha_{EI}(\\mathbf{u_*};\\theta,\\mathcal{D})\\) is inexpensive. In other words, we have a simple analytical formaula for calculating the \\(\\alpha_{EI}(\\mathbf{u_*};\\theta,\\mathcal{D})\\), as it has been provided in Equation (1.19). Secondly, the exact analytical expression of the Equation (1.19) is available. Authors refer to (Rasmussen and Williams 2006) for detail of mathematical formulation. Having the gradient of the function in addition to inexpensive forward function, make the gradient-based method a suitable optimization choice. In this work, the quasi-Newton family of gradient based method, BFGS is used for finding \\(\\mathbf{u}_*^{next}\\). Multi-start BFGS were performed to avoid local optima points (Nocedal and Wright 2006; Byrd et al. 1995). References "],["numerical-example.html", "Chapter 2 Numerical Example 2.1 1-D Toy Problem", " Chapter 2 Numerical Example 2.1 1-D Toy Problem In this section, a 1-D toy problem is considered to illustrate the BO workflow discussed in the previous section. The 1-D problem was selected since it makes it easier to visualize all the workflow steps, hence a better explanation of equations. Though, it can be seen from the 1-D problem that the workflow can easily extend to a higher dimensional problem. The True function to be optimized in this section has an analytical expression with the box constraints, can be shown as: \\[\\begin{equation} \\begin{aligned} &amp; \\underset{u}{\\text{maximize}} &amp; &amp; \\mathbf{J(u)} = 1-\\frac{1}{2} \\left(\\frac{\\sin (12\\mathbf{u})}{1+\\mathbf{u}} + 2\\cos(7\\mathbf{u})\\mathbf{u}^5 + 0.7 \\right) \\\\ &amp; \\text{subject to} &amp; &amp; 0 \\leq \\mathbf{u} \\leq 1 \\end{aligned} \\tag{2.1} \\end{equation}\\] Since the analytical expression of function is available and being a 1-D problem, the global optimum of the function had been found at \\(\\mathbf{u}_M = 3.90\\). The plot of the function and the optimum point has been shown in the Figure ??. The function in the plot has some local optimum around \\(\\mathbf{u}=0.75\\). Choosing a 1-D problem with a non-convex structure was purposeful in this example, in order to see whether BO avoids local optima and converges to a global one or not. However, it is worth mentioning that the exact analytical expression of the objective function in many real-world problems is not available (black-box optimization). What is available is a sample of \\(\\mathbf{U}\\) and \\(\\mathbf{J(U)}\\), represented as \\(\\mathcal{D}=[\\mathbf{U},\\mathbf{J(U)}]\\). Therefore, in the 1-D example, in order to mimic the real world case, we sample a few points to form our \\(\\mathcal{D}\\). We know the analytical expression of the objective function and global optimum of the objective function in hindsight, just for the case we want to compare the performance of BO workflow. To form \\(\\mathcal{D}=[\\mathbf{U},\\mathbf{J(U)}]\\) as Equation (1.4), a sample of five points, \\(\\mathbf{U}=[0.05,0.2,0.5,0.6,0.95]\\) was selected to initialize the workflow. This \\(\\mathbf{U}\\) vector with their correspondent \\(\\mathbf{J(U)}\\), forms the \\[\\mathcal{D}=[\\mathbf{U},\\mathbf{J_U}]=[[0.05,0.2,0.5,0.6,0.95];[0.38, 0.36, 0.77,0.44, 0.16]]\\]. In the upper plot of Figure 2.1, green points in diamond shape show the \\(\\mathcal{D}\\). Then, we can find the \\(\\theta^*\\) through performing optimizing in Equation (1.11) (as it only needs \\(\\mathcal{D})\\). Having \\(\\theta^*\\), we can find the mean value of function \\(\\mathbf{J(u^*)}\\) through Equation (1.12). This mean values (\\(\\mathbf{\\mu_{u_\\ast}}\\)) for each \\(\\mathbf{u^*}\\) have been depicted with a red line in Figure 2.1. The blue lines in 2.1 represents 100 samples of \\(\\mathbf{J_{u_*}}\\) from the gaussian distribution with mean and variance defined at (1.12) at each \\(\\mathbf{u^*}\\). The grey area represents the 95% confidence interval. At this stage, we completed step 1 of the BO. The first point to infer from the upper plot at Figure 2.1 is that there is no uncertainty on the points in \\(\\mathcal{D}\\). The reason for this is (as was discussed in the previous section), here we consider “noise-free” observations. Also, worth mentioning that we have a wider grey area (more uncertainty) in the areas that are more distant from the observations, simply meaning uncertainty is less in points close to observation points. When it comes to “extrapolation,” meaning in the areas outside of the range of observation points, the probabilistic model shows interesting behavior on those “extreme” area (say for example two points at \\(\\mathbf{u^*=0}\\) and \\(\\mathbf{u^*=1}\\) ), the mean curve tend to move toward the mean of all observation points , here it is \\(\\text{average}\\left(\\mathbf{J(U)}\\right)=0.42\\). Suggesting the model shows the mean-reversion behavior when it comes to extrapolation. The lower plot at Figure 2.1, shows the plot of utility function- Equation (1.17) - at each \\(\\mathbf{u^*}\\) value. As the plot suggests, the utility function (\\(\\alpha_{EI}\\)) will have a multi-modal structure. Meaning the optimization process needs a multi-start gradient method (as mentioned in last part of previous section). After performing optimization of Equation (1.20), the blue vertical dotted line shows the \\(\\mathbf{u}_*^{next}=0.46\\) which is the point where the utility function, is maximum. Then this \\(\\mathbf{u}_*^{next}\\) is feed into the true objective function in (2.1), and the pair of \\([(\\mathbf{u}_*^{next}, \\mathbf{J}(\\mathbf{u}_*^{next})]\\) is added to the initial data set, leaving \\[\\mathcal{D}= \\mathcal{D}\\: \\cup[\\mathbf{u}^{next},\\mathbf{J(u}^{next}]=[[0.05,0.2,0.5,0.6,0.95,0.46];[0.38, 0.36, 0.77,0.44, 0.16, 0.91]]\\]. We complete step 2 of the workflow at this stage, and we performed the first iteration of BO. Looking again to the lower figure at Figure 2.1, the utility has two modes around two sides of point \\(\\mathbf{u_*}=0.5\\), say \\(\\mathbf{u_{0.5}^+}=0.5 + \\epsilon\\) and \\(\\mathbf{u_{0.5}^-}=0.5-\\epsilon\\), however the point \\(\\mathbf{u_{0.5}^-}\\) is selected as the next query point. Readers can be referred to the upper plot and it is clear that there is more uncertainty around point \\(\\mathbf{u_{0.5}^-}\\) than \\(\\mathbf{u_{0.5}^+}\\) (while their mean values are the same, due to symmetry around \\(\\mathbf{u_*}=0.5\\)). The utility function always looking for the point that not only maximizes the mean value but is also interested in the points that have higher variance - Equation (1.17) -, which is the case between two points \\(\\mathbf{u_{0.5}^+}\\) and \\(\\mathbf{u_{0.5}^-}\\). Figure 2.1: Ite1 - Top: Gaussian posterior over the initial sample points; Lower: Utility function over the x values If we call Figure 2.1 as iteration # 1, now we can go back to step 1 of BO workflow and start iteration # 2 with new \\(\\mathcal{D}\\). In Figure 2.2 another two iterations have been provided. In each row, the plot on the left represents the plot of posterior written in Equation (1.12), the right shows the utility function provided at Equation (1.13). Note that in Figure 2.2 all axis labels , and legend were removed, to have better visibility. (more info about each plot can be found in 2.1). Interesting to see that in this example case, at iteration #2, the workflow query the point \\(\\mathbf{u}^{next}=0.385\\) which presents the best point so far found through BO workflow. Therefore, after just two iterations, we are around \\(\\frac{x_{best}}{x_{M}}=\\frac{0.385}{0.390}=98.7%\\) of the global optima. Although this is the case for the 1-D problem, it clearly shows the workflow’s strength to approach the global optima in as few iterations as possible. In this case after iteration#2, the total number of times, the true objective function has been evaluated is \\(\\text{size}(\\mathcal{D}) + \\text{size}(total iteration) = 5 + 2=7\\). Figure 2.2: Gaussian posterior of over the initial sample points Before applying the same workflow at the field scale, the 1-D example presented here offers another valuable feature of the BO workflow. Looking at 2.2, we can see that the maximum of the utility function is at the iteration # 3 is in order of \\(10^{-6}\\) . That shows that after optimization, even the best point to be evaluated with an expensive function has very little utility. So we can safely stop the process, since querying points to be sampled from the expensive function has a negligible potential to improve our search in optimization. "],["references.html", "References", " References "]]
